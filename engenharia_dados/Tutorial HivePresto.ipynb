{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo\n",
    "\n",
    "O objetivo deste tutorial é conhecer um pouco mais sobre processamento de dados em HDFS. Utilizaremos o Hive e o Presto para entender a diferença entre dois modelos de computação distribuída.\n",
    "\n",
    "# Hive\n",
    "\n",
    "Para começar iremos explorar o formato utilizado pelo hive para armazenamento de arquivos. Como os dados são armazenados, consultados e como podemos aproveitar o particionamento para executar operações de forma mais eficiente.\n",
    "\n",
    "Utilizaremos um sample de dados de twitter para executar as análises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5,70306133677761E+017</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5,70301130888122E+017</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5,70301083672814E+017</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5,70301031407624E+017</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5,70300817074463E+017</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id        user  retweet_count  \\\n",
       "0  5,70306133677761E+017     cairdin              0   \n",
       "1  5,70301130888122E+017    jnardino              0   \n",
       "2  5,70301083672814E+017  yvonnalynn              0   \n",
       "3  5,70301031407624E+017    jnardino              0   \n",
       "4  5,70300817074463E+017    jnardino              0   \n",
       "\n",
       "                                                text  tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.          NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...          NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...          NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...          NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...          NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \\\n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)   \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)   \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)   \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)   \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  \n",
       "2   neutral  \n",
       "3  negative  \n",
       "4  negative  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# imprime um sample dos dados\n",
    "tweets = pd.read_csv('/home/class/Desktop/hands-on/day2/data/tweets.csv', nrows=10)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armazene o arquivo tweets.json no HDFS:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -mkdir /user/class/tweets_csv/\n",
    "hdfs dfs -copyFromLocal /home/class/Desktop/hands-on/day2/data/tweets.csv /user/class/tweets_csv/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 class class    3031610 2017-06-07 16:33 /user/class/tweets_csv/tweets.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# esse diretorio esta pronto para ser interpretado como uma tabela hive\n",
    "p = subprocess.Popen(['hdfs', 'dfs', '-ls', '/user/class/tweets_csv/'], stdout=subprocess.PIPE)\n",
    "print(p.communicate()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. AlreadyExistsException(message:Table tweets_csv already exists)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# criar uma tabela no hive\n",
    "table = '''CREATE EXTERNAL TABLE tweets_csv (\n",
    "    id VARCHAR(50),\n",
    "    username VARCHAR(50),\n",
    "    retweet INT,\n",
    "    text VARCHAR(200),\n",
    "    coord VARCHAR(500),\n",
    "    dt DATE,\n",
    "    location VARCHAR(500),\n",
    "    timezone VARCHAR(500),\n",
    "    sentiment VARCHAR(50)\n",
    ") ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "LOCATION 'hdfs:///user/class/tweets_csv';\n",
    "'''\n",
    "p = subprocess.Popen(['hive', '-e', table], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "out = p.communicate()\n",
    "print(out[0].decode('utf-8'))\n",
    "print(out[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.532 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# criar uma tabela orc particionada\n",
    "cmd = '''CREATE EXTERNAL TABLE tweets (\n",
    "    id VARCHAR(50),\n",
    "    username VARCHAR(50),\n",
    "    retweet INT,\n",
    "    text VARCHAR(200),\n",
    "    coord VARCHAR(500),\n",
    "    dt DATE,\n",
    "    location VARCHAR(500),\n",
    "    timezone VARCHAR(500),\n",
    "    sentiment VARCHAR(50)\n",
    ") PARTITIONED BY (day DATE)\n",
    "STORED AS ORC\n",
    "LOCATION 'hdfs:///user/class/tweets';\n",
    "'''\n",
    "\n",
    "p = subprocess.Popen(['hive', '-e', cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "out = p.communicate()\n",
    "print(out[0].decode('utf-8'))\n",
    "print(out[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: 2015-02-16\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607163518_4171e475-1cbf-47f4-83ed-0a3c763238f5\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496863666045_0001, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496863666045_0001/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496863666045_0001\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 16:36:00,522 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 16:36:15,421 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.3 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 300 msec\n",
      "Ended Job = job_1496863666045_0001\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-16/.hive-staging_hive_2017-06-07_16-35-18_673_1327851412493257946-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-16)\n",
      "Partition default.tweets{day=2015-02-16} stats: [numFiles=1, numRows=4, totalSize=1980, rawDataSize=3024]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.3 sec   HDFS Read: 3037503 HDFS Write: 2067 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 300 msec\n",
      "OK\n",
      "Time taken: 60.583 seconds\n",
      "\n",
      "Loading data: 2015-02-17\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607163635_69492033-7527-4ebd-ae0c-af5f7cdd3876\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496863666045_0002, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496863666045_0002/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496863666045_0002\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 16:36:47,099 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 16:36:55,547 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.99 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 990 msec\n",
      "Ended Job = job_1496863666045_0002\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-17/.hive-staging_hive_2017-06-07_16-36-35_459_1988706784899262896-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-17)\n",
      "Partition default.tweets{day=2015-02-17} stats: [numFiles=1, numRows=1395, totalSize=93357, rawDataSize=1056015]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.99 sec   HDFS Read: 3037503 HDFS Write: 93449 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 990 msec\n",
      "OK\n",
      "Time taken: 23.656 seconds\n",
      "\n",
      "Loading data: 2015-02-18\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607163713_82752119-22ed-4b14-bbe3-2b33101465b5\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496863666045_0003, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496863666045_0003/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496863666045_0003\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 16:37:23,491 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 16:37:28,787 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.31 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 310 msec\n",
      "Ended Job = job_1496863666045_0003\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-18/.hive-staging_hive_2017-06-07_16-37-13_654_4222053511811343827-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-18)\n",
      "Partition default.tweets{day=2015-02-18} stats: [numFiles=1, numRows=1328, totalSize=88936, rawDataSize=1002548]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.31 sec   HDFS Read: 3037503 HDFS Write: 89028 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 310 msec\n",
      "OK\n",
      "Time taken: 18.402 seconds\n",
      "\n",
      "Loading data: 2015-02-19\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607163740_e496f9a6-0630-4c6d-a45f-bd0389b15150\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496863666045_0004, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496863666045_0004/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496863666045_0004\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 16:37:48,050 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 16:37:55,479 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.75 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 750 msec\n",
      "Ended Job = job_1496863666045_0004\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-19/.hive-staging_hive_2017-06-07_16-37-40_493_7107508669261358956-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-19)\n",
      "Partition default.tweets{day=2015-02-19} stats: [numFiles=1, numRows=1359, totalSize=91038, rawDataSize=1023327]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.75 sec   HDFS Read: 3037503 HDFS Write: 91130 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 750 msec\n",
      "OK\n",
      "Time taken: 18.069 seconds\n",
      "\n",
      "Loading data: 2015-02-20\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607163808_a37af1b1-7762-40be-87cf-e3acf75a8042\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496863666045_0005, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496863666045_0005/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496863666045_0005\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 16:38:18,664 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 16:38:24,052 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.45 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 450 msec\n",
      "Ended Job = job_1496863666045_0005\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-20/.hive-staging_hive_2017-06-07_16-38-08_362_2787953990173158791-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-20)\n",
      "Partition default.tweets{day=2015-02-20} stats: [numFiles=1, numRows=1490, totalSize=98534, rawDataSize=1123460]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.45 sec   HDFS Read: 3037503 HDFS Write: 98626 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 450 msec\n",
      "OK\n",
      "Time taken: 17.546 seconds\n",
      "\n",
      "Loading data: 2015-02-21\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607163832_6fb8a5e9-7f32-4d88-a87f-f9079e994932\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496863666045_0006, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496863666045_0006/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496863666045_0006\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 16:38:40,650 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 16:38:45,972 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.8 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 800 msec\n",
      "Ended Job = job_1496863666045_0006\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-21/.hive-staging_hive_2017-06-07_16-38-32_527_1594774121483337829-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-21)\n",
      "Partition default.tweets{day=2015-02-21} stats: [numFiles=1, numRows=1542, totalSize=102907, rawDataSize=1167421]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.8 sec   HDFS Read: 3037503 HDFS Write: 102999 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 800 msec\n",
      "OK\n",
      "Time taken: 16.338 seconds\n",
      "\n",
      "Loading data: 2015-02-22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607163855_904724fa-5743-49ba-8c8e-eef94990153f\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496863666045_0007, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496863666045_0007/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496863666045_0007\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 16:39:03,398 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 16:39:08,705 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.54 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 540 msec\n",
      "Ended Job = job_1496863666045_0007\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-22/.hive-staging_hive_2017-06-07_16-38-55_412_3727970026048093732-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-22)\n",
      "Partition default.tweets{day=2015-02-22} stats: [numFiles=1, numRows=3064, totalSize=202224, rawDataSize=2325295]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.54 sec   HDFS Read: 3037503 HDFS Write: 202316 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 540 msec\n",
      "OK\n",
      "Time taken: 15.399 seconds\n",
      "\n",
      "Loading data: 2015-02-23\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607163918_8c793d14-bbc9-4ba5-b3b0-3a74f8ef30e3\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496863666045_0008, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496863666045_0008/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496863666045_0008\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 16:39:28,774 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 16:39:35,108 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.58 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 580 msec\n",
      "Ended Job = job_1496863666045_0008\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-23/.hive-staging_hive_2017-06-07_16-39-18_505_6045020454619320744-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-23)\n",
      "Partition default.tweets{day=2015-02-23} stats: [numFiles=1, numRows=3001, totalSize=196397, rawDataSize=2269501]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.58 sec   HDFS Read: 3037503 HDFS Write: 196489 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 580 msec\n",
      "OK\n",
      "Time taken: 18.393 seconds\n",
      "\n",
      "Loading data: 2015-02-24\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607163943_8e042cd0-2e46-4cc5-ac7f-86f885d59023\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496863666045_0009, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496863666045_0009/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496863666045_0009\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 16:39:50,915 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 16:39:56,141 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.23 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 230 msec\n",
      "Ended Job = job_1496863666045_0009\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-24/.hive-staging_hive_2017-06-07_16-39-43_098_9159571508294359738-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-24)\n",
      "Partition default.tweets{day=2015-02-24} stats: [numFiles=1, numRows=1319, totalSize=80590, rawDataSize=997164]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 2.23 sec   HDFS Read: 3037503 HDFS Write: 80681 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 2 seconds 230 msec\n",
      "OK\n",
      "Time taken: 14.725 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converte os dados em csv para formato colunar\n",
    "partitions = ['2015-02-16', '2015-02-17', '2015-02-18',\n",
    "              '2015-02-19', '2015-02-20', '2015-02-21',\n",
    "              '2015-02-22', '2015-02-23', '2015-02-24']\n",
    "\n",
    "for part in partitions:\n",
    "    print('Loading data: %s'%(part))\n",
    "    cmd = '''INSERT OVERWRITE TABLE tweets PARTITION(day='%s')\n",
    "    SELECT * FROM tweets_csv WHERE DATE_FORMAT(dt,'yyyy-MM-dd')='%s';'''%(part, part)\n",
    "    p = subprocess.Popen(['hive', '-e', cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out = p.communicate()\n",
    "    print(out[0].decode('utf-8'))\n",
    "    print(out[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "drwxrwx---   - class class          0 2017-06-07 16:36 /user/class/tweets/day=2015-02-16\n",
      "drwxrwx---   - class class          0 2017-06-07 16:36 /user/class/tweets/day=2015-02-17\n",
      "drwxrwx---   - class class          0 2017-06-07 16:37 /user/class/tweets/day=2015-02-18\n",
      "drwxrwx---   - class class          0 2017-06-07 16:37 /user/class/tweets/day=2015-02-19\n",
      "drwxrwx---   - class class          0 2017-06-07 16:38 /user/class/tweets/day=2015-02-20\n",
      "drwxrwx---   - class class          0 2017-06-07 16:38 /user/class/tweets/day=2015-02-21\n",
      "drwxrwx---   - class class          0 2017-06-07 16:39 /user/class/tweets/day=2015-02-22\n",
      "drwxrwx---   - class class          0 2017-06-07 16:39 /user/class/tweets/day=2015-02-23\n",
      "drwxrwx---   - class class          0 2017-06-07 16:39 /user/class/tweets/day=2015-02-24\n",
      "\n",
      "Found 1 items\n",
      "-rwxrwx---   1 class class       1980 2017-06-07 16:36 /user/class/tweets/day=2015-02-16/000000_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A nova tabela contem os dados particionados por dia\n",
    "p = subprocess.Popen(['hdfs', 'dfs', '-ls', '/user/class/tweets/'], stdout=subprocess.PIPE)\n",
    "print(p.communicate()[0].decode('utf-8'))\n",
    "p = subprocess.Popen(['hdfs', 'dfs', '-ls', '/user/class/tweets/day=2015-02-16'], stdout=subprocess.PIPE)\n",
    "print(p.communicate()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternando entre o Presto e o Hive\n",
    "\n",
    "Para rodar o presto, execute os seguintes comandos (em terminais distintos).\n",
    "\n",
    "```\n",
    "cd ~/presto-server-0.149/\n",
    "./bin/launcher run\n",
    "\n",
    "sudo service hive-metastore stop\n",
    "hive --service metastore\n",
    "\n",
    "presto\n",
    "> use hive.default;\n",
    "```\n",
    "\n",
    "Para executar o hive:\n",
    "\n",
    "```\n",
    "sudo service hive-metastore start (com o outro metastore desligado)\n",
    "hive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute os seguintes experimentos\n",
    "\n",
    "1. Faça consultas filtrando por partições e avalie a diferença no tempo de processamento.\n",
    "2. Faça consultas de agregação e note a diferença nos tempos de processamento entre o ORC e o CSV.\n",
    "3. Faça consultas de agregação variando o número de colunas e avaliando o que ocorre com o tempo de processamento.\n",
    "4. Experimente outros formatos de armazenamento no hive.\n",
    "5. Tente criar um algoritmo de trending topics utilizando o presto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
