{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo\n",
    "\n",
    "O objetivo deste tutorial é conhecer um pouco mais sobre processamento de dados em HDFS. Utilizaremos o Hive e o Presto para entender a diferença entre dois modelos de computação distribuída.\n",
    "\n",
    "# Hive\n",
    "\n",
    "Para começar iremos explorar o formato utilizado pelo hive para armazenamento de arquivos. Como os dados são armazenados, consultados e como podemos aproveitar o particionamento para executar operações de forma mais eficiente.\n",
    "\n",
    "Utilizaremos um sample de dados de twitter para executar as análises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5,70306133677761E+017</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5,70301130888122E+017</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5,70301083672814E+017</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5,70301031407624E+017</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5,70300817074463E+017</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_id        user  retweet_count  \\\n",
       "0  5,70306133677761E+017     cairdin              0   \n",
       "1  5,70301130888122E+017    jnardino              0   \n",
       "2  5,70301083672814E+017  yvonnalynn              0   \n",
       "3  5,70301031407624E+017    jnardino              0   \n",
       "4  5,70300817074463E+017    jnardino              0   \n",
       "\n",
       "                                                text  tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.          NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...          NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...          NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...          NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...          NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \\\n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)   \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)   \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)   \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)   \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)   \n",
       "\n",
       "  sentiment  \n",
       "0   neutral  \n",
       "1  positive  \n",
       "2   neutral  \n",
       "3  negative  \n",
       "4  negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# imprime um sample dos dados\n",
    "tweets = pd.read_csv('~/Desktop/tweets.csv', nrows=10)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armazene o arquivo tweets.json no HDFS:\n",
    "\n",
    "```\n",
    "hdfs dfs -mkdir /user/class/tweets_csv/\n",
    "hdfs dfs -copyFromLocal ~/Desktop/tweets.csv /user/class/tweets_csv/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   1 class class    3031610 2017-06-07 13:57 /user/class/tweets_csv/tweets.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# esse diretorio esta pronto para ser interpretado como uma tabela hive\n",
    "p = subprocess.Popen(['hdfs', 'dfs', '-ls', '/user/class/tweets_csv/'], stdout=subprocess.PIPE)\n",
    "print(p.communicate()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.613 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# criar uma tabela no hive\n",
    "table = '''CREATE EXTERNAL TABLE tweets_csv (\n",
    "    id VARCHAR(50),\n",
    "    username VARCHAR(50),\n",
    "    retweet INT,\n",
    "    text VARCHAR(200),\n",
    "    coord VARCHAR(500),\n",
    "    dt DATE,\n",
    "    location VARCHAR(500),\n",
    "    timezone VARCHAR(500),\n",
    "    sentiment VARCHAR(50)\n",
    ") ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "LOCATION 'hdfs:///user/class/tweets_csv';\n",
    "'''\n",
    "p = subprocess.Popen(['hive', '-e', table], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "out = p.communicate()\n",
    "print(out[0].decode('utf-8'))\n",
    "print(out[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.29 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# criar uma tabela orc particionada\n",
    "cmd = '''CREATE EXTERNAL TABLE tweets (\n",
    "    id VARCHAR(50),\n",
    "    username VARCHAR(50),\n",
    "    retweet INT,\n",
    "    text VARCHAR(200),\n",
    "    coord VARCHAR(500),\n",
    "    dt DATE,\n",
    "    location VARCHAR(500),\n",
    "    timezone VARCHAR(500),\n",
    "    sentiment VARCHAR(50)\n",
    ") PARTITIONED BY (day DATE)\n",
    "STORED AS ORC\n",
    "LOCATION 'hdfs:///user/class/tweets';\n",
    "'''\n",
    "\n",
    "p = subprocess.Popen(['hive', '-e', cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "out = p.communicate()\n",
    "print(out[0].decode('utf-8'))\n",
    "print(out[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: 2015-02-16\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607144352_62824fbc-f7ca-433b-a1df-603d0b097c29\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496845734730_0014, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496845734730_0014/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496845734730_0014\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 14:44:03,823 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 14:44:10,263 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.31 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 310 msec\n",
      "Ended Job = job_1496845734730_0014\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-16/.hive-staging_hive_2017-06-07_14-43-52_938_8916311154467836160-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-16)\n",
      "Partition default.tweets{day=2015-02-16} stats: [numFiles=1, numRows=4, totalSize=1980, rawDataSize=3024]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.31 sec   HDFS Read: 3037503 HDFS Write: 2067 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 310 msec\n",
      "OK\n",
      "Time taken: 19.354 seconds\n",
      "\n",
      "Loading data: 2015-02-17\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607144420_69ca3ef2-48d2-42af-9aaf-3d74ca95a6f6\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496845734730_0015, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496845734730_0015/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496845734730_0015\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 14:44:28,995 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 14:44:35,401 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.73 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 730 msec\n",
      "Ended Job = job_1496845734730_0015\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-17/.hive-staging_hive_2017-06-07_14-44-20_550_327787074054828295-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-17)\n",
      "Partition default.tweets{day=2015-02-17} stats: [numFiles=1, numRows=1395, totalSize=93357, rawDataSize=1056015]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.73 sec   HDFS Read: 3037501 HDFS Write: 93449 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 730 msec\n",
      "OK\n",
      "Time taken: 17.833 seconds\n",
      "\n",
      "Loading data: 2015-02-18\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607144446_43c29b34-c75e-402c-a1e7-a87f134d08a6\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496845734730_0016, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496845734730_0016/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496845734730_0016\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 14:44:56,349 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 14:45:02,713 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.62 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 620 msec\n",
      "Ended Job = job_1496845734730_0016\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-18/.hive-staging_hive_2017-06-07_14-44-46_560_8399724851009573393-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-18)\n",
      "Partition default.tweets{day=2015-02-18} stats: [numFiles=1, numRows=1328, totalSize=88936, rawDataSize=1002548]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.62 sec   HDFS Read: 3037503 HDFS Write: 89028 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 620 msec\n",
      "OK\n",
      "Time taken: 18.348 seconds\n",
      "\n",
      "Loading data: 2015-02-19\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607144512_4c7abef0-4115-4637-93a8-acca360890f3\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496845734730_0017, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496845734730_0017/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496845734730_0017\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 14:45:22,124 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 14:45:28,582 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.27 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 270 msec\n",
      "Ended Job = job_1496845734730_0017\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-19/.hive-staging_hive_2017-06-07_14-45-12_539_7814431938130752336-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-19)\n",
      "Partition default.tweets{day=2015-02-19} stats: [numFiles=1, numRows=1359, totalSize=91038, rawDataSize=1023327]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.27 sec   HDFS Read: 3037503 HDFS Write: 91130 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 270 msec\n",
      "OK\n",
      "Time taken: 18.992 seconds\n",
      "\n",
      "Loading data: 2015-02-20\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607144539_baa3827c-3062-42ab-8d68-5e18a0771859\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496845734730_0018, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496845734730_0018/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496845734730_0018\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 14:45:49,205 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 14:45:55,671 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.54 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 540 msec\n",
      "Ended Job = job_1496845734730_0018\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-20/.hive-staging_hive_2017-06-07_14-45-39_866_8579102962262976105-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-20)\n",
      "Partition default.tweets{day=2015-02-20} stats: [numFiles=1, numRows=1490, totalSize=98534, rawDataSize=1123460]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.54 sec   HDFS Read: 3037503 HDFS Write: 98626 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 540 msec\n",
      "OK\n",
      "Time taken: 17.803 seconds\n",
      "\n",
      "Loading data: 2015-02-21\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607144607_8afe4e58-b491-4680-a428-7e2028764851\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496845734730_0019, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496845734730_0019/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496845734730_0019\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 14:46:15,222 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 14:46:21,600 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.22 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 220 msec\n",
      "Ended Job = job_1496845734730_0019\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-21/.hive-staging_hive_2017-06-07_14-46-07_224_2270769510518921298-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-21)\n",
      "Partition default.tweets{day=2015-02-21} stats: [numFiles=1, numRows=1542, totalSize=102907, rawDataSize=1167421]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.22 sec   HDFS Read: 3037503 HDFS Write: 102999 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 220 msec\n",
      "OK\n",
      "Time taken: 16.171 seconds\n",
      "\n",
      "Loading data: 2015-02-22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607144631_af259d95-472e-4c4e-b82a-9f1a8b309874\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496845734730_0020, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496845734730_0020/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496845734730_0020\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 14:46:40,765 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 14:46:48,280 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.64 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 640 msec\n",
      "Ended Job = job_1496845734730_0020\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-22/.hive-staging_hive_2017-06-07_14-46-31_596_6734104173216111517-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-22)\n",
      "Partition default.tweets{day=2015-02-22} stats: [numFiles=1, numRows=3064, totalSize=202224, rawDataSize=2325295]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.64 sec   HDFS Read: 3037503 HDFS Write: 202316 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 640 msec\n",
      "OK\n",
      "Time taken: 18.601 seconds\n",
      "\n",
      "Loading data: 2015-02-23\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607144658_d8f027b1-d8b9-4a88-b89e-14a11a876512\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496845734730_0021, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496845734730_0021/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496845734730_0021\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 14:47:07,139 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 14:47:13,607 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.5 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 500 msec\n",
      "Ended Job = job_1496845734730_0021\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-23/.hive-staging_hive_2017-06-07_14-46-58_922_7803070850288546323-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-23)\n",
      "Partition default.tweets{day=2015-02-23} stats: [numFiles=1, numRows=3001, totalSize=196397, rawDataSize=2269501]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.5 sec   HDFS Read: 3037503 HDFS Write: 196489 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 500 msec\n",
      "OK\n",
      "Time taken: 16.59 seconds\n",
      "\n",
      "Loading data: 2015-02-24\n",
      "\n",
      "ls: cannot access '/usr/lib/spark/lib/spark-assembly-*.jar': No such file or directory\n",
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "Query ID = class_20170607144723_50737233-e687-4760-8ae0-f256d00cf7d0\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks is set to 0 since there's no reduce operator\n",
      "Starting Job = job_1496845734730_0022, Tracking URL = http://class-VirtualBox:8088/proxy/application_1496845734730_0022/\n",
      "Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1496845734730_0022\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n",
      "2017-06-07 14:47:34,119 Stage-1 map = 0%,  reduce = 0%\n",
      "2017-06-07 14:47:41,562 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.56 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 560 msec\n",
      "Ended Job = job_1496845734730_0022\n",
      "Stage-4 is selected by condition resolver.\n",
      "Stage-3 is filtered out by condition resolver.\n",
      "Stage-5 is filtered out by condition resolver.\n",
      "Moving data to: hdfs://localhost:8020/user/class/tweets/day=2015-02-24/.hive-staging_hive_2017-06-07_14-47-23_467_2324126401141228799-1/-ext-10000\n",
      "Loading data to table default.tweets partition (day=2015-02-24)\n",
      "Partition default.tweets{day=2015-02-24} stats: [numFiles=1, numRows=1319, totalSize=80590, rawDataSize=997164]\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1   Cumulative CPU: 3.56 sec   HDFS Read: 3037503 HDFS Write: 80681 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 3 seconds 560 msec\n",
      "OK\n",
      "Time taken: 21.04 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converte os dados em csv para formato colunar\n",
    "partitions = ['2015-02-16', '2015-02-17', '2015-02-18',\n",
    "              '2015-02-19', '2015-02-20', '2015-02-21',\n",
    "              '2015-02-22', '2015-02-23', '2015-02-24']\n",
    "\n",
    "for part in partitions:\n",
    "    print('Loading data: %s'%(part))\n",
    "    cmd = '''INSERT OVERWRITE TABLE tweets PARTITION(day='%s')\n",
    "    SELECT * FROM tweets_csv WHERE DATE_FORMAT(dt,'yyyy-MM-dd')='%s';'''%(part, part)\n",
    "    p = subprocess.Popen(['hive', '-e', cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out = p.communicate()\n",
    "    print(out[0].decode('utf-8'))\n",
    "    print(out[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 items\n",
      "drwxrwx---   - class class          0 2017-06-07 14:44 /user/class/tweets/day=2015-02-16\n",
      "drwxrwx---   - class class          0 2017-06-07 14:44 /user/class/tweets/day=2015-02-17\n",
      "drwxrwx---   - class class          0 2017-06-07 14:45 /user/class/tweets/day=2015-02-18\n",
      "drwxrwx---   - class class          0 2017-06-07 14:45 /user/class/tweets/day=2015-02-19\n",
      "drwxrwx---   - class class          0 2017-06-07 14:45 /user/class/tweets/day=2015-02-20\n",
      "drwxrwx---   - class class          0 2017-06-07 14:46 /user/class/tweets/day=2015-02-21\n",
      "drwxrwx---   - class class          0 2017-06-07 14:46 /user/class/tweets/day=2015-02-22\n",
      "drwxrwx---   - class class          0 2017-06-07 14:47 /user/class/tweets/day=2015-02-23\n",
      "drwxrwx---   - class class          0 2017-06-07 14:47 /user/class/tweets/day=2015-02-24\n",
      "\n",
      "Found 1 items\n",
      "-rwxrwx---   1 class class       1980 2017-06-07 14:44 /user/class/tweets/day=2015-02-16/000000_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A nova tabela contem os dados particionados por dia\n",
    "p = subprocess.Popen(['hdfs', 'dfs', '-ls', '/user/class/tweets/'], stdout=subprocess.PIPE)\n",
    "print(p.communicate()[0].decode('utf-8'))\n",
    "p = subprocess.Popen(['hdfs', 'dfs', '-ls', '/user/class/tweets/day=2015-02-16'], stdout=subprocess.PIPE)\n",
    "print(p.communicate()[0].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternando entre o Presto e o Hive\n",
    "\n",
    "Para rodar o presto, execute os seguintes comandos (em terminais distintos).\n",
    "\n",
    "```\n",
    "cd ~/presto-server-0.149/\n",
    "./bin/launcher run\n",
    "\n",
    "sudo service hive-metastore stop\n",
    "hive --service metastore\n",
    "\n",
    "presto\n",
    "> use hive.default;\n",
    "```\n",
    "\n",
    "Para executar o hive:\n",
    "\n",
    "```\n",
    "sudo service hive-metastore start (com o outro metastore desligado)\n",
    "hive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute os seguintes experimentos\n",
    "\n",
    "1. Faça consultas filtrando por partições e avalie a diferença no tempo de processamento.\n",
    "2. Faça consultas de agregação e note a diferença nos tempos de processamento entre o ORC e o CSV.\n",
    "3. Faça consultas de agregação variando o número de colunas e avaliando o que ocorre com o tempo de processamento.\n",
    "4. Experimente outros formatos de armazenamento no hive.\n",
    "5. Tente criar um algoritmo de trending topics utilizando o presto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
