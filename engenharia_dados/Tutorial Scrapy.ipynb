{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo\n",
    "\n",
    "O objetivo desse tutorial é introduzí-los a coloeta de dados na web utilizando a ferramenta Scrapy.\n",
    "\n",
    "Vamos mostrar um exemplo simples de coleta utilizando o scrapy já pronto, e etão vocês vão ter de construir suas próprias \"aranhas\" (spiders) e parses para realizarem a coleta em um site com dados do IBGE que criamos especialmente para esssa aula.\n",
    "\n",
    "\n",
    "# Scrapy\n",
    "\n",
    "É um **framework** de código aberto e colaborativo para extração de dados a partir de websites. Ele escala bem, é simples de utilizar e facilmente extensível.\n",
    "\n",
    "## Visão Geral da Arquitetura\n",
    "\n",
    "A figura a seguir mostra uma visão geral da arquitetura do Scrapy com seus componentes e o fluxo dos dados nele [[1]](#[1]-Documentação-sobre-Arquitetura-do-Scrapy).\n",
    "\n",
    "![Arquitetura do Scrapy](imgs/scrapy_architecture_02.png)\n",
    "\n",
    "1. A *Engine* recebe a requisição inicial da Spider para inciar o ***crawl***\n",
    "2. A *Engine* agenda as requisições no ***Scheduler***(Escalonador) e pede a próxima requisição para ***crawl***.\n",
    "3. O Escalonador retorna a próxima Requisição para a *Engine*.\n",
    "4. A Engine enviar as *requisições* ao ***Downloader***, passando pelo ***Downloader Middlewares***.\n",
    "5. Assim que a página foi baixada o *Downloader* gera a Resposta (com aquela página) e envia para a Engine, passando pelo o *Downloader Middlewares*.\n",
    "6. A *Engine* recebe a Resposta do *Downloader* e envia para a ***Spider*** para processamento, passando pela ***Spider Middleware*** .\n",
    "7. A *Spider* processa a Resposta e retorna os items extraídos e um nova Requisição é feita à Engine, passando pela *Spider Middleware*.\n",
    "8. A *Engine* envia os items processados ao ***Item Pipelines***, então envia as Requisições processadas ao Escalonador e pede possíveis novas requisições para *crawl*.\n",
    "9. O processo é repetido (a partir do passo 1) até que não haja requisições do Escalonador.\n",
    "\n",
    "\n",
    "## Rodando nossa primeira Spider\n",
    "\n",
    "No desktop da máquina virtual tem uma pasta ***hands-on***, a mesma contém uma pasta crawler. Vamos acessá-la:\n",
    "\n",
    "```bash\n",
    "$ cd ~/Desktop/hands-on\n",
    "# verifique o conteúdo\n",
    "~/Desktop/hands-on$ ls\n",
    "\n",
    "crawler\n",
    "```\n",
    "Entremos na pasta crawler, nela tem um exemplo simples de uma Spider.\n",
    "\n",
    "```bash\n",
    "~/Desktop/hands-on$ cd crawler\n",
    "~/Desktop/hands-on/crawler$ ls\n",
    "example_spider.py\n",
    "```\n",
    "\n",
    "```bash\n",
    "~/Desktop/hands-on$ cd crawler\n",
    "~/Desktop/hands-on/crawler$ ls\n",
    "example_spider.py\n",
    "```\n",
    "\n",
    "O arquivo **example_spider.py** contém o seguinte código:\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider'\n",
    "    start_urls = ['https://blog.scrapinghub.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for title in response.css('h2.entry-title'):\n",
    "            yield {'title': title.css('a ::text').extract_first()}\n",
    "\n",
    "        for next_page in response.css('div.prev-post > a'):\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```\n",
    "\n",
    "Vamos destrinchar o código acima. Primeiramente, o scrapy já possui uma classe base para criar objetos do tipo ```scrapy.Spider```.\n",
    "Portanto, nós temos de criar uma classe para nossa Spider que herda dessa classe, como está no trecho:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "...\n",
    "```\n",
    " Fazendo isso, nós estamos criando uma classe que respeita a interface esperada para execução do fluxo da arquitetura descrito anteriormente. Para isso, é necessário sobre escrever o método ```parse(self, response)``` da classe ```scrapy.Spider```. \n",
    "\n",
    "```python\n",
    "    def parse(self, response):\n",
    "        for title in response.css('h2.entry-title'):\n",
    "            yield {'title': title.css('a ::text').extract_first()}\n",
    "\n",
    "        for next_page in response.css('div.prev-post > a'):\n",
    "            yield response.follow(next_page, self.parse)\n",
    "``` \n",
    "Esse método é responsável pela leitura da página e extração dos dados. Como podemos ver na chamada do método, esse método recebe como paramentro um response que contém a página web, além disso nos permite percorrer o [DOM do html](https://www.w3schools.com/js/js_htmldom.asp) de forma bem simples (como veremos a seguir).\n",
    "\n",
    "```python\n",
    "    for title in response.css('h2.entry-title'):\n",
    "```\n",
    "\n",
    "O objeto ```response``` possui os Selectors (ou Seletores) [[2]](#[2]-Selectors) que nos ajudam na tarefa de percorrer o DOM do html e extrair os dados elementos espefícos, e isso é possível utilizando expressões XPath ou CSS. O trecho de cigo acima irá retornar todas as ocorrências da tag ```<h2 class='entry-title\">``` da página atual. Para cada ocorrência o código a seguir é executado : \n",
    "```python \n",
    "        yield {'title': title.css('a ::text').extract_first()}\n",
    "```\n",
    "\n",
    "O como ```yield``` irá enviar um item ao ***Item Pipeline*** do Scrapy (como mostrado no fluxo anteriormente). Esse Item pode ser tanto um ```dict``` do python ou um objeto da classe ```scrapy.Item```. Nesse trecho de código, é utilizado o dicionário do python, onde é extraído uma única informação da página que é o texto do link (```<a> algum texto </a>```). São todos o links da página? Não, são apenas links dentro da tag ```<h2 class='entry-title\">``` que é representado pelo objeto ```title```.\n",
    "\n",
    "\n",
    "O com o comando ```yield``` também é possível enviar para o Escalonador novas páginas para serem analisadas. O treco a seguir faz justamente isso.\n",
    "\n",
    "```python\n",
    "           for next_page in response.css('div.prev-post > a'):\n",
    "                yield response.follow(next_page, self.parse)\n",
    "``` \n",
    "\n",
    "Para cada link ```response.css('div.prev-post > a')``` dentro da tag ```<div class=\"prev-post\">``` é utilizado o método follow que vai gerar uma requisição ao Escalonador para cada link e essa requisição utilizará a própria função ```parse``` definida previamente.\n",
    "\n",
    "Mas de qual página ele vai iniciar a varredura? No início da definição de nossa classe é configurado quais serão as urls de partida.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider'\n",
    "    start_urls = ['https://blog.scrapinghub.com']\n",
    "```\n",
    "\n",
    "Além disso, também é uma boa prática definir um nome para nossa Spider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já deu para ter uma ideia de como funciona o Scrapy e como extraimos informações de páginas web com ele, vamos rodar nossa Spider e ver ela em ação.\n",
    "\n",
    "\n",
    "```bash\n",
    "cd ~/Desktop/hands-on/crawler\n",
    "scrapy runspider example_spider.py -o items.json\n",
    "\n",
    "2017-06-07 11:36:59 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n",
    "2017-06-07 11:36:59 [scrapy.utils.log] INFO: Overridden settings: {'FEED_URI': 'items.json', 'FEED_FORMAT': 'json', 'SPIDER_LOADER_WARN_ONLY': True}\n",
    "2017-06-07 11:36:59 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.logstats.LogStats',\n",
    " 'scrapy.extensions.memusage.MemoryUsage',\n",
    " 'scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.feedexport.FeedExporter']\n",
    "2017-06-07 11:36:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2017-06-07 11:36:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2017-06-07 11:36:59 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2017-06-07 11:36:59 [scrapy.core.engine] INFO: Spider opened\n",
    "2017-06-07 11:36:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2017-06-07 11:36:59 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
    "2017-06-07 11:37:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://blog.scrapinghub.com> (referer: None)\n",
    "2017-06-07 11:37:01 [scrapy.core.scraper] DEBUG: Scraped from <200 https://blog.scrapinghub.com>\n",
    "\n",
    "...\n",
    "\n",
    "2017-06-07 11:37:04 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2017-06-07 11:37:04 [scrapy.extensions.feedexport] INFO: Stored json feed (103 items) in: items.json\n",
    "2017-06-07 11:37:04 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 2933,\n",
    " 'downloader/request_count': 11,\n",
    " 'downloader/request_method_count/GET': 11,\n",
    " 'downloader/response_bytes': 123796,\n",
    " 'downloader/response_count': 11,\n",
    " 'downloader/response_status_count/200': 11,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2017, 6, 7, 14, 37, 4, 796248),\n",
    " 'item_scraped_count': 103,\n",
    " 'log_count/DEBUG': 115,\n",
    " 'log_count/INFO': 8,\n",
    " 'memusage/max': 48447488,\n",
    " 'memusage/startup': 48447488,\n",
    " 'request_depth_max': 10,\n",
    " 'response_received_count': 11,\n",
    " 'scheduler/dequeued': 11,\n",
    " 'scheduler/dequeued/memory': 11,\n",
    " 'scheduler/enqueued': 11,\n",
    " 'scheduler/enqueued/memory': 11,\n",
    " 'start_time': datetime.datetime(2017, 6, 7, 14, 36, 59, 917935)}\n",
    "2017-06-07 11:37:04 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio\n",
    "\n",
    "Esse exemplo foi apenas um introdutório a fim de dar uma visão geral do funcionamento do Scrapy. Agora nós iremos coletar os dados que iremos utilizar no restante desse *hands on*. \n",
    "\n",
    "## O site\n",
    "\n",
    "Nós vamos extrair os dados de um site fictício que contém alguns dados do censo do IBGE de 2010. Para acessar esse site, abra navegador e digite na barra de endereço:\n",
    "\n",
    "```http://127.0.0.1:5000```\n",
    "\n",
    "Você deveria ver a seguinte página\n",
    "\n",
    "![Site com Censo 2010](imgs/site.png)\n",
    "\n",
    "Navegue rapidamente nas páginas... Nós estamos interessados nos seguintes dados:\n",
    "\n",
    "![Dados de Interesse](imgs/dados_interesse.png)\n",
    "\n",
    "Clique com botão direito do mouse em qualquer parte da página e deve abrir um popup igual a imagem:\n",
    "\n",
    "![Dados de Interesse](imgs/inspect_element.png)\n",
    "\n",
    "Clique em **Inspect Element (Q)**, ira abrir um inspetor de elementos do navegador. Como exibido a seguir:\n",
    "\n",
    "![Dados de Interesse](imgs/inspetor.png)\n",
    "\n",
    "Com inspetor de elementos nós conseguimos ver a hieraquia do HTML, e assim, pensar em Selector CSS para navegação da Spider pelos links e extração dos dados de interesse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto para Scrapy\n",
    "\n",
    "Vamos criar um projeto Scrapy para fazermos a coleta. Rodando o seguinte comando criamos nosso projeto.\n",
    "```bash\n",
    "# lembre-se de entrar na pasta crawler primeiro\n",
    "cd ~/Desktop/hands-on/crawler\n",
    "\n",
    "# comando para criar um projeto scrapy\n",
    "scrapy startproject ibge\n",
    "```\n",
    "Você deveria obter o seguinte resultado:\n",
    "\n",
    "```bash\n",
    "New Scrapy project 'ibge', using template directory '/home/class/Desktop/hands-on/.venv/lib/python3.5/site-packages/scrapy/templates/project', created in:\n",
    "   /home/class/Desktop/hands-on/crawler/ibgeYou can start your first spider with:\n",
    "   cd ibge\n",
    "   scrapy genspider example example.com\n",
    "```\n",
    "Portanto, nós acabamos de criar um projeto chamado **ibge**. Esse projeto consiste de uma pasta com arquivos referentes ao processo de crawling executado pelo Scrapy. Vamos averiguar a pasta:\n",
    "\n",
    "```bash\n",
    "ibge/\n",
    "    scrapy.cfg\n",
    "    ibge/\n",
    "        __init__.py\n",
    "        middlewares.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "                __init__.py\n",
    "```\n",
    "\n",
    "Como já havíamos falado, o scrapy é altamente flexível desse modo ele nos permite confirguramos (settings.py), e estendermos vários aspectos (pipelines.py e middlewares.py). Além disso, podemos notar que há uma pasta ```spiders/``` e a mesma está vazia (exceto pelo arquivo ```__init__.py```). De fato, nós ainda não temos nenhuma Spider. Precisamos criar uma para fazermos a coleta.\n",
    "\n",
    "Nós temos duas opções para criar uma Spider, 1. podemos fazer na mão e 2. utilizando outro comando o scrapy que gera um Spider padrão. Vamos optar pela segunda:\n",
    "\n",
    "```bash\n",
    "cd ibge\n",
    "scrapy genspider censo 127.0.0.1:5000\n",
    "```\n",
    "\n",
    "Como resultado do comando temos:\n",
    "\n",
    "```bash\n",
    "Created spider 'censo' using template 'basic' in module:\n",
    " ibge.spiders.censo\n",
    " \n",
    "ibge/\n",
    "    scrapy.cfg\n",
    "    ibge/\n",
    "        __init__.py\n",
    "        middlewares.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "                __init__.py\n",
    "                censo.py\n",
    "```\n",
    "\n",
    "Foi gerado um arquivo dentro da pasta ```spiders/``` contendo nossa Spider responsável por coletar os dados do Censo do IBGE do site ```http://127.0.0.1:5000/```. Vamos dar uma olhada no arquivo criado:\n",
    "\n",
    "```python\n",
    "# -- coding: utf-8 --\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class CensoSpider(scrapy.Spider):\n",
    "   name = 'censo'\n",
    "   allowed_domains = ['127.0.0.1:5000']\n",
    "   start_urls = ['http://127.0.0.1:5000/']\n",
    "   \n",
    "   def parse(self, response):\n",
    "       pass\n",
    "```\n",
    "\n",
    "O comando ```scrapy genspider censo 127.0.0.1:5000``` nos gerou uma Spider chamada **censo** e com a url inicial sendo o endereço que passamos **http://127.0.0.1:5000**. Como pode ser visto, nós temos de implementar o método da ```parse``` de acordo com nossa necessidade. Como fora mencionado, é nela que nós definimos como a Spider vai navegar pelos links e extrair as informações de interesse.\n",
    "\n",
    "\n",
    "\n",
    "## Exercício 1\n",
    "\n",
    "Uma vez que temos uma casca para nossa Spider, peço para que você implmentem o método ```parse(slef, response)``` com objetivo de extrair nossas dados de interesse para cada estado, município e área. Os dados de interesse são:\n",
    "\n",
    "![Dados de Interesse](imgs/dados_interesse.png)\n",
    "\n",
    "Além desses exibidos na figura, também queremos o identificador e nome dos estados, municípios e áreas.\n",
    "\n",
    "***Dica 1*** *: Utilizar o arquivo ```example_spider.py``` como espelho*\n",
    "\n",
    "***Dica 2*** *: remover ```allowed_domains =  ['127.0.0.1:5000']``` ou então tire a porta, senão a Spider não vai funcionar muito bem*\n",
    "\n",
    "***Dica 3*** *: Crie outros métodos se necesário*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências\n",
    "\n",
    "###### [1] [Documentação sobre Arquitetura do Scrapy](https://docs.scrapy.org/en/latest/topics/architecture.html)\n",
    "###### [2] [Selectors](https://docs.scrapy.org/en/latest/topics/selectors.html)\n",
    "###### [3] [Documentação Scrapy](https://docs.scrapy.org/en/latest/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
